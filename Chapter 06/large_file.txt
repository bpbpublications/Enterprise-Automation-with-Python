arXiv:2112.09086v1  [stat.ML]  16 Dec 2021Anewlocallylinearembeddingschemeinlightof
Hessianeigenmap
LirenLin
*
andChih-WeiChen
ƒ
DepartmentofAppliedMathematics,NationalSunYat-senUn
iversity,Taiwan
Abstract
WeprovideanewinterpretationofHessianlocallylinearem
bedding(HLLE),
revealingthatitisessentiallyavariantwaytoimplementt
hesameideaoflocally
linearembedding(LLE).Basedonthenewinterpretation,as
ubstantialsimplication
canbemade,inwhichtheideaofﬁHessianﬂisreplacedbyrath
erarbitraryweights.
Moreover,weshowbynumericalexamplesthatHLLEmayproduc
eprojection-like
resultswhenthedimensionofthetargetspaceislargerthan
thatofthedatamanifold,
andhenceonefurthermodicationconcerningthemanifoldd
imensionissuggested.
Combiningalltheobservations,wenallyachieveanewLLE-
typemethod,which
iscalledtangentialLLE(TLLE).Itissimplerandmorerobus
tthanHLLE.
1Introduction

Let
X
=
f
x
i
g
N

i
=1
beacollectionofdatapointsinsome
R
D
.Thegoalofnonlineardimen-
sionalityreduction(ormanifoldlearning)istondfor
X
arepresentation
Y
=
f
y
i
g
N

i
=1
insomelowerdimensional
R
d
,undertheassumptionthat
X
liesonsomeunknownsub-
manifold
M
in
R
D
.
Amongtheseveralexistingmanifoldlearningmethods,Hess
ianeigenmap[2],also
calledHessianlocallylinearembedding(HLLE),isonethat
exhibitsprominentperfor-
manceonthepopularsyntheticdataﬁSwissrollwithaholeﬂ.
Itcanberegardedasa
generalizationofLaplacianeigenmap[1]insomerespector
LLE[4]inanother.How-
ever,itsprocedureconcerningtheconstructionandminimi
zationofﬁHessianﬂismuch
moresophisticated.
Inthispaper,wewillprovideanewinterpretationofthemec
hanismbehindHLLE,
revealingthatwhatitreallydoesfollowsthesameideaasLL
E:Asking
Y
tosatisfy
thelocallinearrelationsfor
X
asbestaspossible.Themaindifferenceslieintheir
waysofdescribingthelocallinearrelations.Roughlyspea
king,HLLEonlytslocal
*
lirenlin2017@gmail.com
ƒ
chencw@math.nsysu.edu.tw
1
linearrelationsofthe
d
-dimensionalprincipalcomponents,andatthesametimeexp
loits
multipleweightstodothisjob.Basedonthisunderstanding
,weareabletomakea
substantialsimplication,inwhichtheideaofﬁHessianﬂi
stotallyabandoned.Indeed,
wewillshowthattheHessianestimatorsinHLLE,whicharesp
eciallydesignedmatrices,
canbereplacedbyratherarbitraryweightmatrices.
Moreover,weobservethatwhenthedimensionofthetargetsp
ace
d
isgreaterthanthat
ofthedatamanifold
M
,anaiveapplicationofHLLEmayresultinatypeofunwanted
result,whichlookslikesomedirectprojectionof
X
onto
R
d
.Suchﬁprojectionpatternsﬂ
arealsoobservedforLLE(seeSection3,or[3]fordetailedd
iscussion).ForHLLE,they
donotappearintheembeddingoftheSwissrollintheplane,f
orwhich
d
=dim
M
=2
.
However,ingeneralamanifoldmaynotbeabletobewellembed
dedinaEuclideanspace
ofthesamedimension,andsetting
d
tobelargerthan
dim
M
mightbemoresuitable.
Insuchasituation,somemodicationhastobemadetoavoidt
heprojectionpatterns.
Combiningthementionedsimplicationandthismodicatio
n,wenallyachieveanew
LLE-typemethod,whichwillbenamedtangentialLLE(TLLE).

2NewinterpretationofHLLE

Asintheintroduction,inthefollowinglet
X
=
f
x
i
g
N

i
=1
beadatasetin
R
D
,whichis
supposedtolieonsomeunknownsubmanifold
M
(sometimesreferredtoasthedata
manifold),andourgoalistondfor
X
arepresentation
Y
=
f
y
i
g
N

i
=1
insomelower
dimensional
R
d
.
WerstreviewtheprocedureofHLLE,givenas(H1)
˘
(H4)below.Theideawillbe
(partly)discussedrightafter.

(H1)Foreach
i
=1
;:::;N
let
U
i
=
f
x
i
1
;:::;x
i
k
gˆXnf
x
i
g
bea
k
-nearestneighbor-
hoodof
x
i
,where
k

1+
d
+
d
(
d
+1)
2
:Forsimplicityweconsider
k
tobeaxednumberforall
i
.
(H2)Let
M
(
i
)
=
h
x
i
1

x
i

x
i
k

x
i
i
2
R
D

k
;where
x
i
=
1
k
P
k

j
=1
x
i
j
.Findthesingularvaluedecomposition
M
(
i
)
=
U
(
i
)

(
i
)
V
(
i
)
T
;where
U
(
i
)
isa
D
Dorthogonalmatrixwhosecolumnswillbedenotedby
u
(
i
)
1
;:::;u
(
i
)
D
,

(
i
)
isa
D
k
diagonalmatrixwithdiagonalentries
˙
(
i
)
1

˙
(
i
)
min(
D;k
)

0
,and
V
(
i
)
isa
k

k
orthogonalmatrixwithcolumns
v
(
i
)
1
;:::;v
(
i
)
k
.
2
(H3)Let
V
(
i
)
d
=[
v
(
i
)
1

v
(
i
)
d
]2
R
k

d
bethematrixconsistingoftherst
d
columns
of
V
(
i
)
,andlet
V
(
i
)
d

V
(
i
)
d
2
R
k

d
(
d
+1)
=
2
bethematrixwhosecolumnsareallthe
vectors
v
(
i
)
s
v
(
i
)
t
,
1

s

t

d
,listedinanyprescribedorder.Here
ab
fortwo
(column)vectors
a
=(
a
1
;:::;a
k
)
and
b
=(
b
1
;:::;b
k
)
denotestheircomponent-
wiseproduct
(
a
1
b
1
;:::;a
k
b
k
)
.Let
1
k
2
R
k
bethevectorwhosecomponentsareall
equalto
1
,andlet
G
(
i
)
=
h
1
k
V
(
i
)
d
V
(
i
)
d

V
(
i
)
d
i
;whichisamatrixofsize
k

(1+
d
+
d
(
d
+1)
2
)
.ApplytheGram-Schmidtprocessto
thecolumnsof
G
(
i
)
toobtainanewmatrixwithorthonormalcolumns,andlet
H
(
i
)
bethesubmatrixofthisnewmatrixconsistingofthelast
d
(
d
+1)
=
2
columns(the
positionof
V
(
i
)
d

V
(
i
)
d
in
G
(
i
)
).
(H4)Set
Y
tobeasolutiontotheminimizationproblem
argmin
f
y
i
g
N

i
=1
ˆ
R
d
N
X
i
=1
k
Y
i
H
(
i
)
k
2
s.t.
YY
T
=
I;
where
Y
=[
y
1

y
N
]2
R
d

N
and
Y
i
=[
y
i
1

y
i
k
]2
R
d

k
.Here
k
A
k
fora
matrix
A
=(
a
ij
)
denotestheFrobeniusnorm
q
P
i;j
a
2

ij
.
BeforegivingournewinterpretationofHLLE,wemakesomepr
eliminaryremarks
andcommentsaboutitsoriginalidea.First,what(H2)doesi
stoperformtheprincipal
componentanalysisfor
U
i
.The
d
-dimensionalprincipalcomponentof
x
i
j
(
j
=1
;:::;k
),
denotedby
e
x
i
j
lateron,istheorthogonalprojectionof
x
i
j
onthe
d
-dimensionalhyper-
plane
T
i
:=
x
i
+
span
(
u
(
i
)
1
;:::;u
(
i
)
d
)
:(1)
Accordingly,wewilluse
e
U
i
todenotetheset
f
e
x
i
1
;:::;
e
x
i
k
g
.Bysettingthecanonical
coordinatesystemon
T
i
inwhich
x
i
istheoriginand
u
(
i
)
1
;:::;u
(
i
)
d
arethedirectionsof
thecoordinateaxes,wecanregard
e
x
i
j
,
j
=1
;:::;k
,asvectorsin
R
d
,whosecoordinates
aregivenbytherst
d
rowsof

(
i
)
V
(
i
)
T
.Thatis,
h
e
x
i
1

e
x
i
k
i
=
2

6

4
˙
(
i
)
1
.
.
.
˙
(
i
)
d
3

7

5
2

6

4
v
(
i
)
T
1
.

.

.
v
(
i
)
T
d
3

7

5
=
2

6

4
˙
(
i
)
1
v
(
i
)
T
1
.

.

.
˙
(
i
)
d
v
(
i
)
T
d
3

7

5
:(2)
The
j
-throw
˙
(
i
)
j
v
(
i
)
T
j
hencerepresentsthe
j
-thcoordinatefunctionon
e
U
i
.
ThegeometricalmotivationofStep(H2)isthat,ifthedimen
sion
d
ofthetargetspace
isthesameasthatofthedatamanifold
M
,then
T
i
isanapproximationofthetangent
spaceat
x
i
.Therefore,
e
U
i
representsthetangentialcomponentof
U
i
.Thismotivation
howeveristotallyignoredintheimplementation,andtheal
gorithmofHLLEcanrun
withoutcheckingwhether
d
=dim
M
.Thus,therecomesaquestion:Isthisequalityin
3
dimensionimportant,orcanitbesafelyignored?Possiblya
bitunexpectedly,itreally
mattersŒfortheoriginalHLLE.Ofcourse,if
d<
dim
M
,itisnosurprisethattherewill
beaheavylossindelity.Whatinterestingisthatchoosing
d>
dim
M
wouldalsocause
HLLEtoproduceunwantedresults.Wewilldiscussaboutthis
issueandgiveasimple
solutiontoitinSection4.Fornowletusassume
d
=dim
M
.
Nowmoveonto(H3).Ourdenitionof
G
(
i
)
in(H3)followstheoriginalpaper[2],
whilesomeresearchers(forexample[5,6])set
G
(
i
)
tobe

1
k
F
(
i
)
F
(
i
)

F
(
i
)

,where
F
(
i
)
:=[
˙
(
i
)
1
v
(
i
)
1

˙
(
i
)
d
v
(
i
)
d
]2
R
k

d
.Innormalsituationswehaverank
(
M
(
i
)
)

d
,and
henceitsrst
d
singularvalues
˙
(
i
)
1
;:::;˙
(
i
)
d
arenonzero.Wewillassumethisthroughout
thepaper.Thustheonlydifferencebetweenthetwodenitio
nsof
G
(
i
)
isthateachcorre-
spondingcolumnmaydifferbyanonzerofactor.Asaconseque
nce,theygiverisetothe
same
H
(
i
)
afterperformingtheGram-Schmidtprocess.
In[2],
H
(
i
)
isclaimedtobeaHessianestimatoron
U
i
,andthemotivationofasking
Y
tosolvetheminimizationproblemin(H4)isbasedonthefact
thatif
Y
canbeobtained
from
X
throughsomeisometricmappingfrom
M
to
R
d
(themostidealsituation),then
theglobalintrinsiccoordinatefunctionson
M
(whicharerepresentedbytherowsof
Y
)shouldhavezerosecondorderderivativeseverywhere.The
seclaimsthemselvesare
worthyofmoreexplanationsanddiscussions.However,wesh
allnotpursuethisdirection
here.Instead,wewillprovideamuchsimplerinterpretatio
nofthemechanismbehind
(H3)and(H4).
Firstletusgobackto(H2).Sincethecolumnsof
M
(
i
)
havemeanzero,
0=
M
(
i
)
1
k
=
U
(
i
)

(
i
)
V
(
i
)
T
1
k
:Then,since
U
(
i
)
isinvertible,andsincetherst
d
singularvalues
˙
(
i
)
1
;:::;˙
(
i
)
d
areas-
sumedtobenonzero,
V
(
i
)
T
d
1
k
=0
.Thus,therst
1+
d
vectors
1
k
;v
(
i
)
1
;:::;v
(
i
)
d
in
G
(
i
)
arealreadyorthogonaltoeachother.Asaconsequence,theG
ram-Schmidtprocessin
(H3)doesnotchangethemexceptfornormalizing
1
k
to
1
p
k
1
k
(whichisalsoredundant
asweonlyneedtheresultofthelast
d
(
d
+1)
=
2
columns).
Nowletususe
h
=(
h
1
;:::;h
k
)
T
todenoteanyoneofthe
d
(
d
+1)
=
2
columnsof
H
(
i
)
.Byconstruction
h
isaunitvectorperpendicularto
1
k
;v
(
i
)
1
;:::;v
(
i
)
d
.Inparticular,it
isperpendicularto
˙
(
i
)
1
v
(
i
)
1
;:::;˙
(
i
)
d
v
(
i
)
d
.From(2),thislaststatementcanbewrittenas
h
e
x
i
1

e
x
i
k
i
2

6

4
h
1
.

.

.
h
k
3

7

5
=0
;whichisnothingbutalinearrelationon
e
U
i
:
h
1
e
x
i
1
+

+
h
k
e
x
i
k
=0
:(h1)
Thus,theuseof
H
(
i
)
isthateachofitscolumnsdescribesalinearrelationon
e
U
i
.Withall
ofthecolumns,itthenprovidesatotalof
d
(
d
+1)
=
2
linearrelations.Alltogetherthey
4
canbeexpressedas
h
e
x
i
1

e
x
i
k
i
H
(
i
)
=0
:(3)
Ontheotherhand,thefactsthat
h
isperpendicularto
1
k
andthat
h
isaunitvectoramount
totwoconstraintsonthecoefcients:
k
X
j
=1
h
j
=0
;(h2)
k
X
j
=1
h
2

j
=1
:(h3)
Fromtheabovepointofview,thepurposeofStep(H4)isalsoc
lear:Itsimplyasks
Y
tosatisfythesamelocallinearrelationsas(3)asbestaspo
ssible.ThuswhatHLLE
does,inthisnewinterpretation,isreallyinthesameveina
sLLE,onlythattheirwaysof
describinglocallinearrelationsaredifferent(seeSecti
on3forsomecomparisons).
Now,forconvenienceletuscallanyvector
(
h
1
;:::;h
k
)
whichsatises(h1)
˘
(h3)an
h
-weight.Anessentialsimplicationwhichcanbemadefromt
heabovenewunderstand-
ingofHLLEisthatwemaychoose
H
(
i
)
arbitrarilyaslongasitscolumnsareconstituted
by
h
-weights!Itneedsnotbeconstructedfromthespeciallydes
igned
V
(
i
)
d

V
(
i
)
d
in(H3),
andalsoitsnumberofcolumnsneedsnotbe
d
(
d
+1)
=
2
.Thenumberofcolumnsin
H
(
i
)
,callit
m
,justindicateshowmany
h
-weightswewouldliketousetocharacterize
thelinearrelationon
e
U
i
.Itisanumberwecandecideatwill,subjectonlytothesligh
tre-
striction
k

1+
d
+
m
(replacingthestrongerrequirement
k

1+
d
+
d
(
d
+1)
=
2
given
in(H1)),sincethevectors
1
k
;v
(
i
)
1
;:::;v
(
i
)
d
andthecolumnsof
H
(
i
)
havetobelinearly
independent.Inotherwords,
wehavetochoose
k

d
+2
in(H1),andthen
m
canbeanynumberbetween
1
;2
;:::;k

d

1
.
Toimplementthenewidea,itisstillconvenienttomakeuseo
ftheGram-Schmidt
processtogenerate
h
-weights.TheonlymodicationfromtheoriginalHLLEistha
twe
canreplace
V
(
i
)
d

V
(
i
)
d
byothermatrices,aslongasthecolumnsof
G
(
i
)
formalinearly
independentset.Forthispurpose,itisreasonabletoresor
ttoarandomconstruction.
Precisely,wereplace(H3)bythefollowing:
ŁSelect
m
randomvectors
r
1
;:::;r
m
in
R
k
,andperformtheGram-Schmidtprocess
tothecolumnsof
G
(
i
)
:=
h
1
k
v
(
i
)
1

v
(
i
)
d
r
1

r
m
i
toobtainanewmatrixwithorthonormalcolumns.Thenset
H
(
i
)
2
R
k

m
tobethe
submatrixconsistingofthelast
m
columns.
5
Figure1:NumericalresultsforSwissrollwithaholebyHLLE
(
k
=8
)andTLLE(
k
=8
;
m
=2
and
m
=1
).
ThenewmethodwillbecalledtangentialLLE(TLLE).Westres
sagainthathereweonly
considercaseswith
d
=dim
M
.InSection4wewilladdonefurthermodicationin
ordertocoveralsocaseswith
d>
dim
M
,andawrittendownalgorithmforthatnal
versionwillbegiventhere.
Wealsostressthat
k

d
+2
isaminimumrestrictioninprinciple.Fromexperiments,
larger
k
isusuallyneeded.Ontheotherhand,muchfewer
h
-weightsthan
k

d

1
may
workprettywell.Nevertheless,itlookslikeusingmultipl
eweights,i.e.
m

2
,iscrucial
toensuregoodresults.Figure1showssomenumericalsimula
tionsontheSwissrollwith
ahole(
d
=2
).Eachofthesimulationsadopts
k
=8
.ForTLLE,largestpossible
m
is
8

2

1=5
,while
m
=2
alreadygivesequallygoodresultasHLLE(whichcan
beregardedasusing
m
=3
).Infact,theoutcomesofHLLEandTLLE(
m
=2
)look
identical!(Thetwopicturesdifferonlyinsize,whichisme
relyaconsequenceofscaling.)
Thisisnotacoincidencebutisusuallythecasefromourrepe
atedexperiments.Though
astonishingatrstsight,thisphenomenonsimplyreectst
hefactthatbothresultsare
almostperfect:Theybothunfoldthe3Ddatafaithfullywith
outmessingupanypart.
3ComparisonofLLEandTLLE

Inthissectionwealsoconsider
d
=dim
M
.WehavementionedthatTLLE(inparticular
HLLE)implementsessentiallythesameideaasLLE:Asking
Y
totthelocallinear
relationsof
X
asbestaspossible.Themaindifferenceslieintheirwaysof
describing
locallinearrelations.Inthissectionwetakeacloselooka
tsomeofthedifferences.
ReaderswhowouldliketoknowthepracticalusageofTLLEqui
cklymayreadonlypoint
(A)belowtohaveabasicunderstandingofﬁprojectionpatte
rnﬂ,andthengostraightto
thenextsection.
ForconveniencewegiveareviewoftheprocedureofLLErst.
Itgoesasfollows:
(L1)Foreach
i
=1
;:::;N
let
U
i
=
f
x
i
1
;:::;x
i
k
gˆXnf
x
i
g
bea
k
-nearestneighbor-
hoodof
x
i
.Forsimplicityweconsider
k
tobeaxednumberforall
i
.
6
(L2)Oneach
U
i
,solvetheminimizationproblem
argmin
(
w
1
;:::;w
k
)
2
R
k
k
x
i

k
X
j
=1
w
j
x
i
j
k
2
s.t.
k
X
j
=1
w
j
=1
;(P1)
andlet
w
(
i
)
=(
w
(
i
)
1
;:::;w
(
i
)
k
)
beasolution.
(L3)Set
Y
tobeasolutionto
argmin
f
y
i
g
N

i
=1
ˆ
R
d
N
X
i
=1
k
y
i

k
X
j
=1
w
(
i
)
j
y
i
j
k
2
s.t.
YY
T
=
I;
where
Y
=[
y
1

y
N
]2
R
d

N
.
Recallthatwecallavector
h
=(
h
1
;:::;h
k
)
thatsatises(h1)
˘
(h3)intheprevious
sectionan
h
-weight.Similarly,letuscallasolution
w
=(
w
1
;:::;w
k
)
ofProblem(P1)
abovea
w
-weight.Itgivesrisetoa(possiblyapproximate)linearre
lationoftheform
x
i
ˇ
w
1
x
i
1
+
w
2
x
i
2
+

+
w
k
x
i
k
:(4)
FollowingaresomedifferencesbetweenLLEandTLLE.
(A)ThelocallinearrelationsinLLEtakethefull
D-dimensionaldata
U
i
intoconsider-
ation,whilethoseinTLLEonlyconcerntheﬁtangentialcomp
onentﬂ
e
U
i
.Thisisa
fundamentaldifference,whichprotectsTLLEfromproducin
gatypeofunwanted
resultcalledﬁprojectionpatternﬂ.Sucharesultlookslik
esomedirectprojection
of
Xˆ
R
D
ontoa
d
-dimensionalsubspace.Theyareobservedintheapplicatio
n
ofLLEwhenthelocallinearrelations(4)forall
i
=1
;:::;N
areexactorhighly
approximatelytrue.Thecorereasonisthattherelations(4
)arepreservedbyany
global(i.e.independentof
i
)lineartransformationfrom
R
D
to
R
d
(see[3]formore
details).Bycontrast,relationsoftheform(h1)donothave
thisproperty:
h
1
e
x
i
1
+

+
h
k
e
x
i
k
6)
h
1
f
Ax
i
1
+

+
h
k
f
Ax
i
k
;where
A
:R
D
!
R
d
denotesagenerallinearmapindenepdentof
i
,and
f
f
Ax
i
1
;:::
;f
Ax
i
k
g
denotesthetangentialcomponent(
d
-dimensionalprincipalcomponent)of
f
Ax
i
1
;:::;Ax
i
k
g
.
(B)AnotherimportantpointwhichmakestheperformanceofT
LLEmuchbetterthan
thatofLLEisthatTLLEcanexploitmultiple
h
-weightstodescribelinearrelations
on
e
U
i
,whileinLLEonlyonelinearrelationdescribedby
w
-weightisconsideredfor
U
i
.Infact,theperformanceofTLLEisevidentlyworsewhenusi
ngonly
m
=1
(as
Figure1shows).Notehoweverthattheideaofusingmultiple
weightstoimprove
LLEisnotnew.Seeforexample[7].
7
(C)Inthelinearrelation(4),
x
i
issomethingregardedastheﬁcenterﬂof
U
i
,despitethe
factthatitmayactuallybefarfromthecentergeometricall
y(forexamplewhen
x
i
isapointclosetotheboundaryof
M
).Ontheotherhand,intherelation(h1)
thereisnopointwhichisregardedasplayingthecentralrol
e.Nevertheless,this
differenceissomewhatsupercialandmightbeoflesssigni
cance.Forexample,
ifsay
h
1
6=0
in(h1),thenbydividing(h1)and(h2)by
h
1
(andabandoningthe
size-controllingconstraint(h3)),weget
e
x
i
1
=
e
w
2
e
x
i
2
+

+
e
w
k
e
x
i
k
e
w
2
+

+
e
w
k
=1
;where
e
w
j
:=

h
j
=h
1
.ThuswegetalinearrelationofLLEtype,with
e
x
i
1
playing
theroleofthecenter.Similarly,itisalsoeasytorewritea
nLLE-typelinearrelation
intothatofaTLLE-typebymultiplyingasuitablefactor.
Beforeendingthissection,wewouldliketoshareonemorein
terestingobservation
abouttheselectionof
k
-nearestneighborhood.InLLE,thereasonweexclude
x
i
from
U
i
isclear,otherwisewemayobtainthetriviallinearrelatio
n
x
i
=
x
i
fromProblem(P1).
ThispracticeisinheritedbyHLLE.However,acasualexamin
ationofHLLEorTLLE
revealsnoharmtoinclude
x
i
in
U
i
.Forexample,wemayset
x
i
1
tobe
x
i
,and
x
i
2
;:::;x
i
k
are
k

1
nearestpointsto
x
i
.However,indoingsoasubtleproblemoccurs:wemay
have
U
i
=
U
j
forsome
i
6=
j
.Indeed,weobservedthatsuchanequalityoccurswithhigh
probabilityandinabundance,whichcausesalotofredundan
cysincetheinformationin
H
(
i
)
and
H
(
j
)
mayoverlap.Asaconsequence,itisstillagoodpracticetoe
xclude
x
i
from
U
i
inTLLE.
4Amodicationforcaseswith
d>
dim
M
Forconvenienceletususe
d
M
todenotethedimensionof
M
inthissection,and
d
is
reservedforthedimensionofthetargetspace.Asismention
ed,intheoriginalideaof
HLLE
d
issupposedtobethesameas
d
M
,whichhoweverisignoredinthealgorithm.
Indeed,itwouldbebettertohavenosucharestriction,sinc
eingeneralamanifoldwith
nontrivialcurvaturecannotbewellembeddedin
R
d
for
d

d
M
,andselectingalarger
d
mightsometimesbemoresuitable.Ofcourse,thisperspecti
veisbasedonthewish
topreserveasmuchmetricinformationoftheoriginaldata
X
aspossible.Inanother
direction,whenadenitelylowdimensionaltargetspaceis
stronglypreferredsuchasfor
visualizationofdatain
R
2
or
R
3
,onemayevenconsider
d<d
M
.Inthiscasesubstantial
lossindelityisunavoidable.Wewillnotdiscussthisdire
ction.
Backtotheassumption
d
=
d
M
.Nowthequestionis:Doesitreallymatter?Unfortu-
nately,theanswerisyes.Infact,anaiveapplicationofHLL
Ewith
d>d
M
mayresultin
ﬁprojectionpatternsﬂasLLE(seepoint(A)inSection3).Nu
mericalillustrationswillbe
8
givenintheendofthissection.Thereasonforgettingthese
unwantedresultsshouldbe
that,sinceeach
U
i
isalreadyclosetoa
d
M
-dimensionalpiece,its
d
-dimensionalprincipal
componentcontainsalmostthefull
D-dimensionalinformation.Inotherwords,alinear
relation(h1)for
e
x
i
1
;:::;
e
x
i
k
isnearlyanexactlinearrelationfor
x
i
1
;:::;x
i
k
,andhence
isalmostpreservedbyanylinearmapfrom
R
D
to
R
d
.ForLLE,itisthenrecommended
in[3]thatoneusesregularizationtodisturbthelinearrel
ationssothattheywillnotbe
ﬁtooexactﬂ.HereforHLLE,wehaveanothersimplesolution:
adheretotheruleofﬁt-
tingonlythetangentpartﬂ.Thatis,when
d
M
<d
,weshouldtlinearrelationsofthe
d
M
-dimensionalprincipalcomponent,insteadofthe
d
-dimensionalone.
Specically,weset
e
U
i
=
f
e
x
i
1
;:::;
e
x
i
k
g
tobethe
d
M
-dimensionalprincipalcompo-
nentof
U
i
.Accordingly,an
h
-weightfor
e
U
i
isaunitvectorin
R
k
whichisperpendicular
tothe
d
M
+1
vectors
1
k
;v
(
i
)
1
;:::;v
(
i
)
d
M
.Therestrictionofthenumber
k
thenbecomes
k

d
M
+2
,whichallowsatmost
k

d
M

1
linearlyindependent
h
-weightsforeach
e
U
i
.Ontheotherhand,sincewestillwanttondarepresentatio
n
Y
of
X
in
R
d
,the
minimizationproblemin(H4)isunchanged.Wesummarizethe
algorithmforthisnal
versionofTLLEbelow.
TLLEAlgorithm
Inputs:
(1)
X
=
f
x
i
g
N

i
=1
ˆ
R
D
,(2)twopositiveintegers
d
M
and
d
,with
1

d
M

d<D
,(3)apositiveinteger
k

d
M
+2
,and(4)apositiveinteger
m

k

d
M

1
.
Output:
Y
=
f
y
i
g
N

i
=1
ˆ
R
d
.
Procedure:
1.Foreach
i
=1
;:::;N
let
U
i
=
f
x
i
1
;:::;x
i
k
gˆXnf
x
i
g
bea
k
-nearest
neighborhoodof
x
i
.
2.Let
M
(
i
)
=[
x
i
1

x
i

x
i
k

x
i
]2
R
D

k
,where
x
i
=
1
k
P
k

j
=1
x
i
j
,and
computeitssingularvaluedecomposition:
M
(
i
)
=
U
(
i
)

(
i
)
V
(
i
)
T
:Let
v
(
i
)
1
;:::;v
(
i
)
d
M
betherst
d
M
columnsof
V
(
i
)
.
3.Select
m
randomvectors
r
1
;:::;r
m
in
R
k
,andperformtheGram-Schmidt
processtothecolumnsofthematrix
h
1
k
v
(
i
)
1

v
(
i
)
d
M
r
1

r
m
i
toobtainanewmatrix.Thenset
H
(
i
)
2
R
k

m
tobethesubmatrixofthisnew
matrixconsistingofthelast
m
columns.
9
4.Set
Y
tobeasolutiontotheminimizationproblem
argmin
f
y
i
g
N

i
=1
ˆ
R
d
N
X
i
=1
k
Y
i
H
(
i
)
k
2
s.t.
YY
T
=
I;
where
Y
=[
y
1

y
N
]2
R
d

N
and
Y
i
=[
y
i
1

y
i
k
]2
R
d

k
.Notethatthe
norm
kk
formatricesdenotestheFrobeniusnorm.
Someremarksaboutthepracticalimplementationareinorde
r.
Remark
1
.
Herewerecalltheroutinewaytosolvetheminimizationprob
leminStep
4.First,rewritethesummation
P
N

i
=1
k
Y
i
H
(
i
)
k
2
intoasingle
k
YH
k
2
,where
H
isan
N

mN
matrixdenedasfollows:For
i
=1
;:::;N
,itssubmatrixofthe
((
i

1)
m
+1)
-
th,
((
i

1)
m
+2)
-th,...,
((
i

1)
m
+
m
)
-thcolumnsandthe
i
1
-th,
i
2
-th,...,
i
k
-throwsis
H
(
i
)
;otherentriesareallzero.Thenanyminimizer
f
y
i
g
N

i
=1
isgivenby
h
y
1

y
N
i
T
=
h
g
1

g
d
i
;where
g
1
;:::;g
d
areorthonormaleigenvectorsof
HH
T
correspondingtothesmallest
d
eigenvalues(countingmultiplicity).Weassumethattheir
correspondingeigenvaluesare
alreadyarrangedinascendingorder.However,inthisway
g
1
issupposedtobe
1
p
N
1
N
,
aneigenvectorof
HH
T
correspondingtothesmallesteigenvalue
0
.Thiseigenvectoris
redundantforourpurposeofembedding
X
into
R
d
,andcanbeomitted.Alternatively,
thisomissioncanberegardedasaddingonefurtherconstrai
nt
Y
1
N
=0
totheminimiza-
tionproblem,meaningthatweask
f
y
i
g
N

i
=1
tohavemeanzero.Therefore,whatisreally
executedinthealgorithmissetting
g
1
;:::;g
d
tobethe2ndtothe
(
d
+1)
-theigenvalues.
Asdiscussedin[3](forLLE),thispracticemaybeproblemat
iciftherearemorethanone
linearlyindependenteigenvectorscorrespondingtotheei
genvalue
0
.ForTLLE,sucha
problemwouldalmostneveroccuraslongasmultipleweights
isadopted.Since
H
has
size
N

mN
andisconstructedrandomly,
1
N
issafelytheonlyeigenvectorof
HH
T
correspondingto
0
if
m

2
.
Remark
2
.
Inapplicationswhenthedimensionof
M
isnotwellunderstoodinadvance,
d
M
canberealizedasthenumberofsignicantsingularvaluesi
n

(
i
)
.Althoughthis
criterionsoundssomewhatimprecise,fromourexperiencei
tisusuallyeasytomakea
judgment,atleastforarticialexamples.Forexample,aty
picalneighborhood
U
i
forthe
Swissrollmayhave
˙
(
i
)
1
=3
:1475
,
˙
(
i
)
2
=2
:4907
and
˙
(
i
)
3
=0
:2346
.Itisobviousthat
thersttwoaresignicantandthethirdoneisnot,showingt
hatthedatamanifoldistwo-
dimensional.Wemaylookatthisissuefromanotherangle:If
X
issuchthatthereshows
nodeniteﬁnumberofsignicantsingularvaluesﬂfromprin
cipalcomponentanalysison
theneighborhoods
U
i
,thentheﬁmanifoldperspectiveﬂon
X
wouldbequestionable,and
applyingTLLEtoitmaynotproduceareliableresult.
10
Figure2:AcomparisonbetweenTLLEandHLLEontheTrefoilkn
ot.
Figure3:ResultsofTLLEandHLLEontheSwissrollwithahole
thatisisometricallyembeddedin
R
9
,
andthenmappedbackto
R
3
.
Finally,wewouldliketogivesomenumericalexamplestodem
onstratehowTLLE
canavoidprojectionpatternswhileHLLEcannot.Forthispu
rpose,wehavetoconsider
threedifferentdimensions:
d
M
<d<D
.Thesimplestcasethatisvisualizableis
d
M
=1
,
d
=2
,and
D=3
.Thismeansthatthedatamanifoldisaspacecurve,and
wearetondarepresentationforitontheplane.Figure2sho
wsnumericalresultsfor
applyingTLLEandHLLEtotheso-calledﬁTrefoilknotﬂ.Note
thatinthiscase,although
thedimensionofthemanifoldisone,itcannotbeembeddedin
therealline.Weseethat
TLLErealizestheknotasatopologicalcircle,whileHLLEpr
ojects(possiblywithsome
distortion)theknotontotheplane,causingself-intersec
tions.
Figure3showsanotherillustrativeexample.Itisobtained
byrstperformingan
isometricalembeddingoftheSwissrollwithaholein
R
9
,andthenŒsupposewedonot
knowthatitcanbewellunfoldedontheplaneŒapplyingTLLEa
ndHLLEtomapit
backto
R
3
.Fromtheknowledgeof
d
M
=2
,TLLEiscapableofidentifyingtheintrinsic
geometry,andunfoldstheSwissrolltosomeextentin
R
3
.Ontheotherhand,HLLEjust
11
performssomeprojectionfrom
R
9
to
R
3
.Usuallytheresultsaresimilartotheoriginal3D
dataandlooknotbad,whileinafewcasestheyhappentobehig
hlycompressedalong
somedirectionandarenotsatisfactory.

5Conclusion

InthispaperweexplainedthatHLLEcanbeviewedasimplemen
tingthesameideaas
LLE:Askingthedimension-reduceddata
f
y
i
g
N

i
=1
ˆ
R
d
tosatisfythesamelocallinear
relationsastheoriginaldata
f
x
i
g
N

i
=1
ˆ
R
D
asbestaspossible.Themaindifferenceslie
inthefollowingtwopoints:
(A)HLLEonlyconsiderslinearrelationsofthe
d
-dimensionalprincipalcomponentfor
eachneighborhood;
(B)HLLEexploitsmultiple(originally
d
(
d
+1)
=
2
)linearrelationsforeachneighbor-
hood.
Withthesebeingclear,weproposedasimplicationwhereth
eﬁHessianestimatorﬂwas
replacedbyrandomlyconstructedweightmatrices.Moreove
r,when
d
isgreaterthan
d
M
(thedimensionofthedatamanifold),HLLEmayproduceproje
ction-likeresults.Toavoid
thisproblemwesuggestedthatthe
d
-dimensionalprincipalcomponentin(A)shouldbe
replacedbythe
d
M
-dimensionalone,whichrepresentsthetangentialcompone
ntofthe
originaldata.Combiningtheabovesimplicationandmodi
cation,wenallyachieved
anewLLE-typemethodwhichwasnamedtangentiallocallylin
earembedding(TLLE).
ItissimplerandmorerobustthanHLLE.
Sofar,ournumericalexperimentsarefocusedonarticiald
atasetssuchastheSwiss
rollwithahole,andtheperformancesofTLLElookexcellent
.Whetheritisalsohelpful
inproducingreliableresultsforrealworlddataordatawit
hnoiseisanimportantdirection
forfutureinvestigation.

Acknowledgment

ThisworkissupportedbyMinistryofScienceandTechnology
ofTaiwanundergrant
numberMOST110-2636-M-110-005-.TheauthorsthankDr.Hau
-TiengWuforreading
andgivingvaluablecommentsontherstversionofourmanus
cript.
References

[1]MikhailBelkinandParthaNiyogi.Laplacianeigenmapsf
ordimensionalityreduction
anddatarepresentation.
Neuralcomputation
,15(6):1373Œ1396,2003.
12
[2]DavidL.DonohoandCarrieGrimes.Hessianeigenmaps:Lo
callylinearembed-
dingtechniquesforhigh-dimensionaldata.
ProceedingsoftheNationalAcademyof
Sciences
,100(10):5591Œ5596,2003.
[3]LirenLin.Avoidingunwantedresultsinlocallylineare
mbedding:Anewunder-
standingofregularization.
arXiv
,2108.12680,2021.
[4]SamT.RoweisandLawrenceK.Saul.Nonlineardimensiona
lityreductionbylocally
linearembedding.
Science
,290(5500):2323Œ2326,2000.
[5]JianzhongWang.
Geometricstructureofhigh-dimensionaldataanddimensio
nality
reduction
.Springer,2012.
[6]QiangYeandWeifengZhi.Discretehessianeigenmapsmet
hodfordimensionality
reduction.
JournalofComputationalandAppliedMathematics
,278:197Œ212,2015.
[7]ZhenyueZhangandJingWang.Mlle:Modiedlocallylinea
rembeddingusingmul-
tipleweights.In
Advancesinneuralinformationprocessingsystems
,pages1593Œ
1600,2007.
13
